{
  "hash": "69ce46841dcfc9bcb619da722fdcb752",
  "result": {
    "markdown": "---\ntitle: \"Monte Carlo integration\"\nauthor: \"Dr. Alexander Fisher\"\nexecute:\n  warning: true\nformat: \n    revealjs:\n      smaller: true\n---\n\n\n# Concept\n\n## Statistical motivation\n\nIntegration is everywhere,\n\n-   CDF\n\n-   Computing means, medians\n\n-   Computing marginal probabilities\n\n... and more\n\n## Monte Carlo integration\n\n-   Approximate a deterministic integral by a stochastic average\n\n-   Shines when other methods of integration are impossible (e.g. high dimensional integration)\n\n-   Works because of law of large numbers: for a random variable $X$, the sample mean $\\bar{x}_N$ converges to the true mean $\\mu$ as the number of samples $N$ tends to infinity.\n\n. . .\n\nRecall\n\n\n$$\n\\mathbb{E}[X] = \\int_x x~f_x(x) dx \\approx \\frac{1}{N} \\sum_i^N x_i \n$$\n\n\nwhere $f_x(x)$ is the probability density function for random variable $x$.\n\n. . .\n\n#### Key insight\n\n-   Expectations are integrals **and** integrals are expectations.\n\n# Toy example: estimating $\\pi$\n\n## The picture to have in mind\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](lec16_files/figure-revealjs/unnamed-chunk-2-1.png){width=960}\n:::\n:::\n\n\n-   The area of the unit circle, $A_\\text{circle} = \\pi$. We'll pretend we don't know and want to estimate $A_\\text{circle}$\n\n-   50 random points thrown into the box defined by coordinates (-1, -1), (-1, 1), (1, -1), (1, 1)\n\n-   39 points land inside, 11 points outside\n\n-   $\\frac{39}{50} \\approx \\frac{A_\\text{circle}}{A_\\text{box}}$\n\n-   $A_{\\text{circle}} \\approx .78 \\cdot 4 = 3.12$\n\n## How is this Monte Carlo integration?\n\n\n$$\n\\begin{aligned}\n\\pi &= \\int_0^1 \\int_0^{2\\pi} r d\\theta dr\\\\\n&= \\int \\int I(x^2 + y^2 < 1) dx dy\\\\\n&= 4 \\mathbb{E}~I(X^2 + Y^2 < 1)\\\\\n&\\approx 4\\cdot\\frac{1}{n}\\sum_{i=1}^n I(X^2 + Y^2  < 1)\n\\end{aligned}\n$$\n\n\nwhere $X$ and $Y$ are iid $Uniform(-1, 1)$ and $I()$ is the indicator function.\n\n<!-- ## Seen another way -->\n\n<!-- We also know, analytically, that the area of the unit circle, $A_c=\\frac{\\pi}{2}$. Supposing we don't know this, we could write our integration, -->\n\n<!-- $$ -->\n<!-- 2 \\int_{-1}^1 \\sqrt{(1 - x^2)}dx = A_c -->\n<!-- $$ -->\n<!-- We can write this as an expectation, -->\n\n<!-- $$ -->\n<!-- \\begin{aligned} -->\n<!-- A_c &= 2 \\cdot \\mathbb{E}~f(x_i)\\\\ -->\n<!-- &= 2\\cdot  \\int_{-1}^1 g(x)~f_x(x) dx\\\\ -->\n<!-- &= 2 \\cdot \\int_{-1}^1 g(x) \\cdot \\frac{1}{2} -->\n<!-- \\end{aligned} -->\n<!-- $$ -->\n<!-- where $X \\overset{\\mathrm{iid}}{\\sim} Uniform(-1, 1)$  and $g(x) = \\sqrt{(1 - x^2)}$. -->\n\n## Exercise\n\n1.  Approximate $\\pi$ in R using 5000 samples $(x, y)$ that are iid $Uniform(-1, 1)$.\n\n2.  Plot the estimate of $\\pi$ on the y-axis and the number of samples on the $x-axis$.\n\n-   Repeat, the two steps above using samples from 5000 samples $(x, y)$ that are iid $Uniform(-5, 5)$\n\n\n\n## Why Monte Carlo?\n\nConsider computing the integral\n\n\n$$\nV = \\int h(x) p(x) dx\n$$ where $p(x)$ is a density.\n\n\n-   We could perform numerical integration by computing the Rieman sum of $h(x)p(x)$ over some fine grid of $x$.\n\n-   If $h(x)p(x)$ is bounded, then a grid of mesh size $\\delta > 0$ gives an $\\mathcal{O}(\\delta)$ approximation error.\n\n-   If $x$ is one dimensional, then a grid of mesh size $\\delta$ requires $N = \\frac{1}{\\delta}$ many grid points.\n\n. . .\n\n-   If $x$ has dimension $p$, then we need $N = \\left(\\frac{1}{\\delta}\\right)^p$ grid points.\n\n. . .\n\nBy CLT, $\\hat{V}_n = \\frac{1}{N} \\sum_{i = 1}^N h(X_i)$ where $X_i \\overset{\\mathrm{iid}}{\\sim} p(x)$ converges to $V$ at the rate $\\sqrt{N}$, irrespective of $p$!\n\n## The curse (blessing) of dimensionality  ![](images/Screen%20Shot%202023-03-24%20at%201.45.21%20AM.png)\n\nFigure from [Betancourt, Michael. \"A conceptual introduction to Hamiltonian Monte Carlo.\" arXiv preprint arXiv:1701.02434 (2017).](https://arxiv.org/pdf/1701.02434.pdf)\n\n# Naive Monte Carlo\n\n## Example\n\nConsider computing the integral\n\n\n$$\nV = \\int_1^3 x~e^{-\\frac{(x-4)^2}{2}} \\frac{1}{\\sqrt{2\\pi}} e^{-\\frac{x^2}{2}} dx\n$$\n\nHere, $p(x)$ is the standard normal density and \n\n\n$$\nh(x) = I(x \\in (1, 3))~x~e^{-\\frac{(x-4)^2}{2}}\n$$\n\n\n. . . \n\nSo we can view this as $\\mathbb{E}~h(x) \\approx \\frac{1}{N} \\sum_{i=1}^N h(x)$ where $X$ is standard normal.\n\n\n## $p(x)$ off from $h(x)$\n\n- Note: here $h(x) = x~e^{-\\frac{(x-4)^2}{2}} \\frac{1}{\\sqrt{2\\pi}}$ (i.e. I am dropping the indicator function for illustrative purposes)\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](lec16_files/figure-revealjs/unnamed-chunk-4-1.png){width=960}\n:::\n:::\n\n## Samples not good for $h(x)p(x)$\n\n:::panel-tabset\n\n## plot\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](lec16_files/figure-revealjs/unnamed-chunk-6-1.png){width=960}\n:::\n:::\n\n## code\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(2)\nhp = function(x) {\n  h(x) * dnorm(x)\n}\n\nN = 50\npoints = data.frame(x = rnorm(N, 0, 1),\n                    y = rep(0, N))\n\nggplot() +\n xlim(-4, 8) +\n  geom_function(fun = hp) +\n  geom_point(data = points, aes(x = x, y = y)) +\n  labs(x = \"x\", y = \"\") +\n  theme_minimal() + \n  labs(title = \"h(x)p(x)\") +\n  stat_function(fun = hp, \n                xlim = c(1,3),\n                geom = \"area\",\n                fill = \"steelblue\", alpha = 0.5)\n```\n:::\n\n:::\n\n## Resulting in large approximation error\n\n::: panel-tabset\n\n## plot\n\n::: {.cell}\n::: {.cell-output-display}\n![](lec16_files/figure-revealjs/unnamed-chunk-10-1.png){width=960}\n:::\n:::\n\n\n## code\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(magrittr)\nset.seed(2)\nN = 1000\nx = rnorm(N, 0, 1)\n\nh1 = function(x) {\nz = h(x)\nz[x < 1] = 0\nz[x > 3] = 0\n  return(z)\n}\n\nestimate = vector(length = N)\nfor (n in 1:N) {\n  estimate[n] = mean(h1(x[1:n]))\n}\n\nV = data.frame(x = seq(N),\n               y = estimate)\n\nV %>%\n  ggplot(aes(x = x, y = y)) +\n  geom_line() +\n  theme_minimal() +\n  labs(x = \"N\", y = \"sample mean\") +\n  scale_x_continuous(trans='log10')\n```\n:::\n\n\n:::\n\n<!-- ## A better strategy -->\n\n<!-- - Notice that  -->\n\n<!-- $$ -->\n<!-- V = \\frac{e^{-\\frac{1}{4}}}{\\sqrt{2}} -->\n<!-- \\int_1^3 x \\frac{1}{\\sqrt{2\\pi(1/2)}} -->\n<!-- e^{-\\frac{(x-2)^2}{2(1/2)}} -->\n<!-- $$ -->\n<!-- - So we can approximate $V$ by  -->\n\n<!-- $$ -->\n<!-- \\frac{e^{-\\frac{1}{4}}}{\\sqrt{2}}~\\frac{1}{N}\\sum_{i = 1}^N X_i~I(X_i\\in(1,3)) -->\n<!-- $$ -->\n\n<!-- where $X_i \\overset{\\mathrm{iid}}{\\sim} N(2, (1/\\sqrt{2})^2))$ -->\n\n<!-- ## Comparison -->\n\n<!-- ```{r} -->\n<!-- library(magrittr) -->\n<!-- set.seed(2) -->\n<!-- N = 1000 -->\n<!-- x = rnorm(N, 2, sqrt(1/2)) -->\n\n<!-- hnew = function(x) { -->\n<!-- x[x < 1] = 0 -->\n<!-- x[x > 3] = 0 -->\n<!--   return(x) -->\n<!-- } -->\n\n<!-- C = exp(-1/4) / sqrt(2) -->\n\n<!-- estimate = vector(length = N) -->\n<!-- for (n in 1:N) { -->\n<!--   estimate[n] = mean(hnew(x[1:n])) -->\n<!-- } -->\n\n<!-- estimate = estimate*C -->\n\n<!-- V2 = data.frame(x = seq(N), -->\n<!--                y = estimate) -->\n\n<!-- V %>% -->\n<!--   ggplot(aes(x = x, y = y)) + -->\n<!--   geom_line(aes(color = \"Naive approach\")) +  -->\n<!--   geom_line(data = V2, aes(x = x, y = y, col = \"New sampler\")) + -->\n<!--   theme_minimal() + -->\n<!--   labs(x = \"N\", y = \"sample mean\") + -->\n<!--   scale_x_continuous(trans='log10')  -->\n<!-- ``` -->\n\n\n\n\n## Further reading\n\n-   The paper that launched it all: [Metropolis, Nicholas, and Stanislaw Ulam. \"The monte carlo method.\" Journal of the American statistical association 44.247 (1949): 335-341.](https://web.williams.edu/Mathematics/sjmiller/public_html/105Sp10/handouts/MetropolisUlam_TheMonteCarloMethod.pdf)\n\n-   A better introduction: [Harrison, Robert L. \"Introduction to monte carlo simulation.\" AIP conference proceedings. Vol. 1204. No. 1. American Institute of Physics, 2010.](https://aip.scitation.org/doi/pdf/10.1063/1.3295638?casa_token=GE-qxaAAVGgAAAAA:F3aHIjQWDj45QwH6VL_B6D6FzPRdiiEjVx4NLgWAZ7QAg-_RPyh_-KrvQWEdgsTkysMgEk5Ib_g7)\n\n-   [A Monte Carlo algorithm to estimate a median](https://blogs.sas.com/content/iml/2018/02/21/monte-carlo-estimate-median.html) by Rick Wicklin at SAS\n\n## Acknowledgements\n\nThe best parts of these slides were adapted from Prof. Surya Tokdar's notes on Monte Carlo integration.\n",
    "supporting": [
      "lec16_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    function fireSlideChanged(previousSlide, currentSlide) {\n\n      // dispatch for htmlwidgets\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for reveal\n    if (window.Reveal) {\n      window.Reveal.addEventListener(\"slidechanged\", function(event) {\n        fireSlideChanged(event.previousSlide, event.currentSlide);\n      });\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}